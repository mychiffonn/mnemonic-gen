# DPO (Direct Preference Optimization) Configuration
train:
  # Training hyperparameters
  learning_rate: 3e-5
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.1
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  optim: "paged_adamw_8bit"
  max_grad_norm: 0.1

  # DPO specific parameters
  beta: 0.1 # Temperature parameter for DPO
  loss_type: "sigmoid" # Loss type: "sigmoid" or "hinge"
  label_smoothing: 0.0 # Label smoothing factor

  # Generation parameters
  max_prompt_length: 1000
  max_completion_length: 800

  # Trainer parameters
  logging_steps: 20
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 1
  num_train_epochs: 2

  # Output
  report_to: "wandb" # Options: "wandb", "none"
  output_dir: "outputs"

  # eval
  per_device_eval_batch_size: 16
  eval_strategy: "steps"
  eval_steps: 200

  # Save strategy
  save_strategy: "steps"
  save_steps: 200
  save_total_limit: 2
  load_best_model_at_end: true

# Data configuration
data:
  train_dataset: "chiffonng/en-vocab-mnemonics-dpo" # Updated for DPO format
  test_dataset: "chiffonng/en-vocab-mnemonics-test"

# Push configuration
push:
  quantization_methods: ["q4_k_m", "f16"]
  save_methods: ["merged_4bit"]
